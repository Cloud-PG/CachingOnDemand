{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"On-Demand XCache cluster What's XCache XCache description is available in this article here . You can look at the official XrootD documentation for detailed information about the XRootD tool: basic configuration cmsd configuration proxy file cache XCache components The setup infrastructure is shown in Fig. B8, where the clients that run the payload can be instructed to request data to a cache system deployed on the same cloud provider and thus with low latency. The cache stack consists in: a proxy server to function as bridge between the private network of the cache and the client. This server will simply tunnel the request from cache servers. a cache redirector for federating each cache server deployed. If a new server is added, it will be automatically configured to contact this redirector for registration a configurable number of cache servers, the core of the tool that are responsibles for reading-ahead from remote site while caching. This setup has been tested on different cloud providers. It is also been tested at a scale of 2k concurrent jobs on Open Telekom Cloud resources in the context of HelixNebulaScience Cloud [6] project. In the context of the eXtreme Data-Cloud project [7], a collection of recipes have been produced for the automatic deployment of a cache service on demand using different automation technology. For bare metal installation an Ansible [8] playbook is available that can deploy either directly on host or through docker container the whole stack. For those who use docker swarm for container orchestration a docker-compose [9] recipe is also available as for Kubernetes where an Helm [10] chart is provided. All these solutions have been integrated in DODAS [11] and thus with very few changes the same setup can be automatically replicated in different kind of cloud resources. On-demand XCache deployment with docker compose Please follow the instruction here Deployment on Kubernetes with Helm helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update helm install -n cache-cluster cloudpg/cachingondemand More details in this demo Ansible deployment Requirements Ansible 2.4 OS: Centos7 valid CMS /etc/vomses Port: one open service port Valid grid host certifate Valid service certificate that is able to read from remote origin (to be stored in /etc/grid-security/xrd/xrdcert.pem, /etc/grid-security/xrd/xrdkey.pem) Role Variables BLOCK_SIZE: 512k # size of the file block used by the cache CACHE_LOG_LEVEL: info # server log level CACHE_PATH: /data/xrd # folder for cached files CACHE_RAM_GB: 12 # amount of RAM for caching in GB. Suggested ~50% of the total HI_WM: 0.9 # higher watermark of used fs space LOW_WM: 0.8 # lower watermark of used fs space N_PREFETCH: 0 # number of blocks to be prefetched ORIGIN_HOST: origin # hostname or ip adrr of the origin server ORIGIN_XRD_PORT: 1094 # xrootd port to contact origin on REDIR_HOST: xcache-service # hostname or ip adrr of the cache redirector REDIR_CMSD_PORT: 31213 # cmsd port of the cache redirector metricbeat_polltime: 60s # polling time of the metricbeat sensor metric_sitename: changeme # sitename to be displayed for monitoring elk_endpoint: localhost:9000 # elasticsearch endpoint url elastic_username: dodas # elasticsearch username elastic_password: testpass # elasticsearch password Example Playbook --- - hosts: localhost remote_user: root roles: - role: dciangot.xcache Deployment example: CMS XCache https://xcache.readthedocs.io/en/latest/automated-grid.html Deployment with DODAS Kubernetes cluster Kubernetes deployment charts Real case example: TOSCA template for XCache in CMS experiment with Marathon","title":"Home"},{"location":"#on-demand-xcache-cluster","text":"","title":"On-Demand XCache cluster"},{"location":"#whats-xcache","text":"XCache description is available in this article here . You can look at the official XrootD documentation for detailed information about the XRootD tool: basic configuration cmsd configuration proxy file cache","title":"What's XCache"},{"location":"#xcache-components","text":"The setup infrastructure is shown in Fig. B8, where the clients that run the payload can be instructed to request data to a cache system deployed on the same cloud provider and thus with low latency. The cache stack consists in: a proxy server to function as bridge between the private network of the cache and the client. This server will simply tunnel the request from cache servers. a cache redirector for federating each cache server deployed. If a new server is added, it will be automatically configured to contact this redirector for registration a configurable number of cache servers, the core of the tool that are responsibles for reading-ahead from remote site while caching. This setup has been tested on different cloud providers. It is also been tested at a scale of 2k concurrent jobs on Open Telekom Cloud resources in the context of HelixNebulaScience Cloud [6] project. In the context of the eXtreme Data-Cloud project [7], a collection of recipes have been produced for the automatic deployment of a cache service on demand using different automation technology. For bare metal installation an Ansible [8] playbook is available that can deploy either directly on host or through docker container the whole stack. For those who use docker swarm for container orchestration a docker-compose [9] recipe is also available as for Kubernetes where an Helm [10] chart is provided. All these solutions have been integrated in DODAS [11] and thus with very few changes the same setup can be automatically replicated in different kind of cloud resources.","title":"XCache components"},{"location":"#on-demand-xcache-deployment-with-docker-compose","text":"Please follow the instruction here","title":"On-demand XCache deployment with docker compose"},{"location":"#deployment-on-kubernetes-with-helm","text":"helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update helm install -n cache-cluster cloudpg/cachingondemand More details in this demo","title":"Deployment on Kubernetes with Helm"},{"location":"#ansible-deployment","text":"","title":"Ansible deployment"},{"location":"#requirements","text":"Ansible 2.4 OS: Centos7 valid CMS /etc/vomses Port: one open service port Valid grid host certifate Valid service certificate that is able to read from remote origin (to be stored in /etc/grid-security/xrd/xrdcert.pem, /etc/grid-security/xrd/xrdkey.pem)","title":"Requirements"},{"location":"#role-variables","text":"BLOCK_SIZE: 512k # size of the file block used by the cache CACHE_LOG_LEVEL: info # server log level CACHE_PATH: /data/xrd # folder for cached files CACHE_RAM_GB: 12 # amount of RAM for caching in GB. Suggested ~50% of the total HI_WM: 0.9 # higher watermark of used fs space LOW_WM: 0.8 # lower watermark of used fs space N_PREFETCH: 0 # number of blocks to be prefetched ORIGIN_HOST: origin # hostname or ip adrr of the origin server ORIGIN_XRD_PORT: 1094 # xrootd port to contact origin on REDIR_HOST: xcache-service # hostname or ip adrr of the cache redirector REDIR_CMSD_PORT: 31213 # cmsd port of the cache redirector metricbeat_polltime: 60s # polling time of the metricbeat sensor metric_sitename: changeme # sitename to be displayed for monitoring elk_endpoint: localhost:9000 # elasticsearch endpoint url elastic_username: dodas # elasticsearch username elastic_password: testpass # elasticsearch password","title":"Role Variables"},{"location":"#example-playbook","text":"--- - hosts: localhost remote_user: root roles: - role: dciangot.xcache","title":"Example Playbook"},{"location":"#deployment-example-cms-xcache","text":"https://xcache.readthedocs.io/en/latest/automated-grid.html","title":"Deployment example: CMS XCache"},{"location":"#deployment-with-dodas","text":"Kubernetes cluster Kubernetes deployment charts Real case example: TOSCA template for XCache in CMS experiment with Marathon","title":"Deployment with DODAS"},{"location":"DOCKER/","text":"USAGE Available options --nogsi : avoid client server gsi auth --nogrid : avoid WLCG CAs installation --health_port : port for healthcheck process listening, type=int, default=80 --config : specify xrootd config file Important container paths /data/xrd/ : saved file location for both cache and std modes /etc/xrootd/ : config files dir /var/log/xrootd/cmsd.log : log of cmsd service /var/log/xrootd/xrootd.log : log of xrootd service Running by hand Just put your xrootd config file in $PWD/config:/etc/xrootd # with your xrd_cache.conf on $PWD/config sudo docker run --rm --privileged -p 32294:32294 -p 31113:31113 -v $PWD/config:/etc/xrootd cloudpg/cachingondemand --config /etc/xrootd/xrd_test.conf REMEMBER To expose the ports indicated in your config file. In the case of config/xrd_test.conf are: 32294, 31113 File saved will be put on /data/xrd, so you may want to mount your storage backend there An health check is available on: # response 0 everything running, response 1 something went wrong curl container_ip /check_health In case of response 1, with docker logs you can see a log dump of the crashed daemon. Local stack deployment with docker compose: Origin+Cache+CacheRedirector You need to first install docker-compose . Then run use the docker-compose.yml file provided to bring up locally: an origin server with config in config/xrd_test_origin.conf a file cache server with config in config/xrd_test.conf a file cache redirector with config in config/xrd_test-redir.conf The command for bringing the full stack up is: git clone https://github.com/Cloud-PG/cachingondemand.git cd cachingondemand/docker /usr/local/bin/docker-compose up -d To shutdown the stack: /usr/local/bin/docker-compose down First functional test # Put a test file on the remote host sudo docker exec -ti docker_client_1 sh -c echo \\ This is my file\\ test.txt xrdcp test.txt root://docker_origin_1:1194//test.txt # Request that file from the cache redirector xrootd process # that is listening on 1094 sudo docker exec -ti docker_client_1 xrdcp -f -d3 root://docker_redirector_1//test.txt remote_test.txt # If you find no error you can now check that the file is correctly cached on cache server sudo docker exec -ti docker_cache_1 ls /data/xrd/ # you should see these two files: test.txt test.txt.cinfo","title":"Quick Start (Docker)"},{"location":"DOCKER/#usage","text":"","title":"USAGE"},{"location":"DOCKER/#available-options","text":"--nogsi : avoid client server gsi auth --nogrid : avoid WLCG CAs installation --health_port : port for healthcheck process listening, type=int, default=80 --config : specify xrootd config file","title":"Available options"},{"location":"DOCKER/#important-container-paths","text":"/data/xrd/ : saved file location for both cache and std modes /etc/xrootd/ : config files dir /var/log/xrootd/cmsd.log : log of cmsd service /var/log/xrootd/xrootd.log : log of xrootd service","title":"Important container paths"},{"location":"DOCKER/#running-by-hand","text":"Just put your xrootd config file in $PWD/config:/etc/xrootd # with your xrd_cache.conf on $PWD/config sudo docker run --rm --privileged -p 32294:32294 -p 31113:31113 -v $PWD/config:/etc/xrootd cloudpg/cachingondemand --config /etc/xrootd/xrd_test.conf REMEMBER To expose the ports indicated in your config file. In the case of config/xrd_test.conf are: 32294, 31113 File saved will be put on /data/xrd, so you may want to mount your storage backend there An health check is available on: # response 0 everything running, response 1 something went wrong curl container_ip /check_health In case of response 1, with docker logs you can see a log dump of the crashed daemon.","title":"Running by hand"},{"location":"DOCKER/#local-stack-deployment-with-docker-compose-origincachecacheredirector","text":"You need to first install docker-compose . Then run use the docker-compose.yml file provided to bring up locally: an origin server with config in config/xrd_test_origin.conf a file cache server with config in config/xrd_test.conf a file cache redirector with config in config/xrd_test-redir.conf The command for bringing the full stack up is: git clone https://github.com/Cloud-PG/cachingondemand.git cd cachingondemand/docker /usr/local/bin/docker-compose up -d To shutdown the stack: /usr/local/bin/docker-compose down","title":"Local stack deployment with docker compose: Origin+Cache+CacheRedirector"},{"location":"DOCKER/#first-functional-test","text":"# Put a test file on the remote host sudo docker exec -ti docker_client_1 sh -c echo \\ This is my file\\ test.txt xrdcp test.txt root://docker_origin_1:1194//test.txt # Request that file from the cache redirector xrootd process # that is listening on 1094 sudo docker exec -ti docker_client_1 xrdcp -f -d3 root://docker_redirector_1//test.txt remote_test.txt # If you find no error you can now check that the file is correctly cached on cache server sudo docker exec -ti docker_cache_1 ls /data/xrd/ # you should see these two files: test.txt test.txt.cinfo","title":"First functional test"},{"location":"demo/DEMO/","text":"DEPLOY CACHINGONDEMAND STACK ON K8S REQUIREMENTS For part 1: Docker Docker compose For part 2: k8s Docker in Docker kubectl and helm installed on the local machine remote xrootd host to connect to service certificate authorized to read from remote xrootd host kept on your local machine as user{cert,key}.pem voms configuration file (optional) kept on your local machine in folder ./vomses PART 1 PLAYGROUND WITH DOCKER AND DOCKER COMPOSE Please follow the instruction here PART 2 START A LOCAL KUBERNETES CLUSTER Let's start a k8s cluster locally with 4 nodes: NUM_NODES=3 k8s-dind up The output should be something like: * Bringing up coredns and kubernetes-dashboard deployment.extensions/coredns scaled deployment.extensions/kubernetes-dashboard scaled .............[done] NAME STATUS ROLES AGE VERSION kube-master Ready master 2m37s v1.13.0 kube-node-1 Ready none 116s v1.13.0 kube-node-2 Ready none 116s v1.13.0 kube-node-3 Ready none 116s v1.13.0 * Access dashboard at: http://127.0.0.1:32769/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy and dashboard should be accessible at the prompted link. Now your kube config file has been update so you should be able to query the cluster by: kubectl get node AUTH N/Z MODEL IN XCACHE STORE CERTIFICATES IN K8S SECRETS Certificates can be saved on k8s and made available to all the cache server with this command: kubectl create secret generic certs --from-file=cert.pem=$PWD/usercert.pem --from-file=key.pem=$PWD/userkey.pem For more details about k8s secrets please visit this page STORE VOMSES IN CONFIGMAP Vomses can be saved on k8s and made available to all the cache server with this command: kubectl create configmap vomses-config --from-file=vomses/ For more details about k8s configMaps please visit this page INSTALL HELM AND CACHINGONDEMAND REPOSITORY Initialize helm on the cluster and then install the CachingOnDemand repository. helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update TAKING A LOOK TO DEPLOYMENT PARAMETERS The deployment can be tune using the Helm following parameters (either passing them via yaml file or via CLI). cache: repository: cloudpg/cachingondemand tag: helm redir_host: xcache-redir-service.default.svc.cluster.local replicas: 1 cache_host_path: /data/xrd block_size: 512k mem_gb: 2 high_wm: 0.95 low_wm: 0.80 n_prefetch: 1 origin_host: cms-xrootd.infn.it origin_xrd_port: 1094 xrdport: 31494 cmsport: 31113 streams: 256 external_ip: 192.168.0.123 vo: http_port: 8080 redirector: repository: cloudpg/cachingondemand tag: helm replicas: 1 xrdport: 31294 cmsport: 31213 proxy: repository: cloudpg/cachingondemand tag: helm replicas: 1 external_ip: 192.168.0.123 xrdport: 31394 DEPLOY THE CLUSTER Let's use the default parameters for now (by default working with CMS remote end point): helm install -n cache-cluster cloudpg/cachingondemand the output of the command should look like: NAME: cache-demo LAST DEPLOYED: Fri May 31 14:57:02 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: == v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xcache-server-service NodePort 10.107.22.234 192.168.0.123 31494:31494/TCP,31113:31113/TCP 0s xcache-redir-service NodePort 10.110.216.224 192.168.0.123 31294:31294/TCP,31213:31213/TCP 0s xcache-proxy NodePort 10.108.121.41 192.168.0.123 31394:31394/TCP 0s == v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE xredir-pod 1 1 1 0 0s proxy-pod 1 1 1 0 0s xcache-pod 1 1 1 0 0s == v1/Pod(related) NAME READY STATUS RESTARTS AGE xredir-pod-8469496c75-mfwmq 0/1 ContainerCreating 0 0s proxy-pod-64c88f5c75-6p9md 0/1 ContainerCreating 0 0s xcache-pod-74b94865b4-tlmgb 0/2 ContainerCreating 0 0s CHECK THE STATUS TEST THE FUNCTIONALITIES CUSTOMIZE THE DEPLOYMENT STANDALONE CACHE SERVER EXAMPLE","title":"Demo on K8s with Heml"},{"location":"demo/DEMO/#deploy-cachingondemand-stack-on-k8s","text":"","title":"DEPLOY CACHINGONDEMAND STACK ON K8S"},{"location":"demo/DEMO/#requirements","text":"For part 1: Docker Docker compose For part 2: k8s Docker in Docker kubectl and helm installed on the local machine remote xrootd host to connect to service certificate authorized to read from remote xrootd host kept on your local machine as user{cert,key}.pem voms configuration file (optional) kept on your local machine in folder ./vomses","title":"REQUIREMENTS"},{"location":"demo/DEMO/#part-1","text":"","title":"PART 1"},{"location":"demo/DEMO/#playground-with-docker-and-docker-compose","text":"Please follow the instruction here","title":"PLAYGROUND WITH DOCKER AND DOCKER COMPOSE"},{"location":"demo/DEMO/#part-2","text":"","title":"PART 2"},{"location":"demo/DEMO/#start-a-local-kubernetes-cluster","text":"Let's start a k8s cluster locally with 4 nodes: NUM_NODES=3 k8s-dind up The output should be something like: * Bringing up coredns and kubernetes-dashboard deployment.extensions/coredns scaled deployment.extensions/kubernetes-dashboard scaled .............[done] NAME STATUS ROLES AGE VERSION kube-master Ready master 2m37s v1.13.0 kube-node-1 Ready none 116s v1.13.0 kube-node-2 Ready none 116s v1.13.0 kube-node-3 Ready none 116s v1.13.0 * Access dashboard at: http://127.0.0.1:32769/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy and dashboard should be accessible at the prompted link. Now your kube config file has been update so you should be able to query the cluster by: kubectl get node","title":"START A LOCAL KUBERNETES CLUSTER"},{"location":"demo/DEMO/#auth-nz-model-in-xcache","text":"","title":"AUTH N/Z MODEL IN XCACHE"},{"location":"demo/DEMO/#store-certificates-in-k8s-secrets","text":"Certificates can be saved on k8s and made available to all the cache server with this command: kubectl create secret generic certs --from-file=cert.pem=$PWD/usercert.pem --from-file=key.pem=$PWD/userkey.pem For more details about k8s secrets please visit this page","title":"STORE CERTIFICATES IN K8S SECRETS"},{"location":"demo/DEMO/#store-vomses-in-configmap","text":"Vomses can be saved on k8s and made available to all the cache server with this command: kubectl create configmap vomses-config --from-file=vomses/ For more details about k8s configMaps please visit this page","title":"STORE VOMSES IN CONFIGMAP"},{"location":"demo/DEMO/#install-helm-and-cachingondemand-repository","text":"Initialize helm on the cluster and then install the CachingOnDemand repository. helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update","title":"INSTALL HELM AND CACHINGONDEMAND REPOSITORY"},{"location":"demo/DEMO/#taking-a-look-to-deployment-parameters","text":"The deployment can be tune using the Helm following parameters (either passing them via yaml file or via CLI). cache: repository: cloudpg/cachingondemand tag: helm redir_host: xcache-redir-service.default.svc.cluster.local replicas: 1 cache_host_path: /data/xrd block_size: 512k mem_gb: 2 high_wm: 0.95 low_wm: 0.80 n_prefetch: 1 origin_host: cms-xrootd.infn.it origin_xrd_port: 1094 xrdport: 31494 cmsport: 31113 streams: 256 external_ip: 192.168.0.123 vo: http_port: 8080 redirector: repository: cloudpg/cachingondemand tag: helm replicas: 1 xrdport: 31294 cmsport: 31213 proxy: repository: cloudpg/cachingondemand tag: helm replicas: 1 external_ip: 192.168.0.123 xrdport: 31394","title":"TAKING A LOOK TO DEPLOYMENT PARAMETERS"},{"location":"demo/DEMO/#deploy-the-cluster","text":"Let's use the default parameters for now (by default working with CMS remote end point): helm install -n cache-cluster cloudpg/cachingondemand the output of the command should look like: NAME: cache-demo LAST DEPLOYED: Fri May 31 14:57:02 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: == v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xcache-server-service NodePort 10.107.22.234 192.168.0.123 31494:31494/TCP,31113:31113/TCP 0s xcache-redir-service NodePort 10.110.216.224 192.168.0.123 31294:31294/TCP,31213:31213/TCP 0s xcache-proxy NodePort 10.108.121.41 192.168.0.123 31394:31394/TCP 0s == v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE xredir-pod 1 1 1 0 0s proxy-pod 1 1 1 0 0s xcache-pod 1 1 1 0 0s == v1/Pod(related) NAME READY STATUS RESTARTS AGE xredir-pod-8469496c75-mfwmq 0/1 ContainerCreating 0 0s proxy-pod-64c88f5c75-6p9md 0/1 ContainerCreating 0 0s xcache-pod-74b94865b4-tlmgb 0/2 ContainerCreating 0 0s","title":"DEPLOY THE CLUSTER"},{"location":"demo/DEMO/#check-the-status","text":"","title":"CHECK THE STATUS"},{"location":"demo/DEMO/#test-the-functionalities","text":"","title":"TEST THE FUNCTIONALITIES"},{"location":"demo/DEMO/#customize-the-deployment","text":"","title":"CUSTOMIZE THE DEPLOYMENT"},{"location":"demo/DEMO/#standalone-cache-server-example","text":"","title":"STANDALONE CACHE SERVER EXAMPLE"}]}