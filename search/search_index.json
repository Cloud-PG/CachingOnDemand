{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"On-Demand XCache cluster What's XCache XCache description is available in this article here . You can look at the official XrootD documentation for detailed information about the XRootD tool: basic configuration cmsd configuration proxy file cache XCache components The setup infrastructure is shown the figure below, where the clients that run the payload can be instructed to request data to a cache system deployed on the same cloud provider and thus with low latency. The cache stack consists in: a proxy server to function as bridge between the private network of the cache and the client. This server will simply tunnel the request from cache servers. a cache redirector for federating each cache server deployed. If a new server is added, it will be automatically configured to contact this redirector for registration a configurable number of cache servers , the core of the tool that are responsibles for reading-ahead from remote site while caching. This setup has been tested on different cloud providers. It is also been tested at a scale of 2k concurrent jobs on Open Telekom Cloud resources in the context of HelixNebulaScience Cloud project. In the context of the eXtreme Data-Cloud project , a collection of recipes have been produced for the automatic deployment of a cache service on demand using different automation technology. For bare metal installation an Ansible playbook is available that can deploy either directly on host or through docker container the whole stack. For those who use docker swarm for container orchestration a docker-compose recipe is also available as for Kubernetes where an Helm chart is provided. All these solutions have been integrated in DODAS and thus with very few changes the same setup can be automatically replicated in different kind of cloud resources. AuthN/Z mode in XCache The client show its identity only to the cache server The cache server will check in its local mapfile if the client is allowed to read the requested namespace If that is the case the cache server will server the file from its disk if already cached or it will use its own certificate (robot/service/power user as needed) to authenticate with the remote storage for the reading process The remote storage check its own mapfile if the robot/service/power user certificate is allowed to read from that namespace. N.B. a procedure to use a user proxy forwarding approach is available but not recomended for security reasons. AuthN/Z mode in XCache with OIDC Coming soon... On-demand XCache deployment with docker compose Please follow the instruction here Deployment on Kubernetes with Helm helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update helm install -n cache-cluster cloudpg/cachingondemand More details in this demo Deployment with DODAS A guided demo is available here Ansible deployment Requirements Ansible 2.4 OS: Centos7 valid CMS /etc/vomses Port: one open service port Valid grid host certifate Valid service certificate that is able to read from remote origin (to be stored in /etc/grid-security/xrd/xrdcert.pem, /etc/grid-security/xrd/xrdkey.pem) Role Variables Create and customize your xcache ansible configuration (e.g. xcache_config.yaml ) xrootd_version: 4.8.3-1.el7 metricbeat_version: 6.2.4-x86_64 monitoring: # enable metricbeat sensors by setting this to \"metricbeat\" path_to_certs: /etc/grid-security/xrd BLOCK_SIZE: 512k # size of the file block used by the cache CACHE_LOG_LEVEL: info # server log level CACHE_PATH: /data/xrd # folder for cached files CACHE_RAM_GB: 12 # amount of RAM for caching in GB. Suggested ~50% of the total HI_WM: \"0.9\" # higher watermark of used fs space LOW_WM: \"0.8\" # lower watermark of used fs space N_PREFETCH: \"0\" # number of blocks to be prefetched ORIGIN_HOST: origin # hostname or ip adrr of the origin server ORIGIN_XRD_PORT: \"1094\" # xrootd port to contact origin on REDIR_HOST: xcache-service # hostname or ip adrr of the cache redirector REDIR_CMSD_PORT: \"31213\" # cmsd port of the cache redirector metricbeat_polltime: 60s # polling time of the metricbeat sensor metric_sitename: changeme # sitename to be displayed for monitoring elk_endpoint: localhost:9000 # elasticsearch endpoint url elastic_username: dodas # elasticsearch username elastic_password: testpass # elasticsearch password Example Playbook First install the role: ansible-galaxy install git+https://github.com/Cloud-PG/CachingOnDemand.git,ansible Then create a basic ansible playbook (e.g. xcache_playbook.yaml ) --- - hosts: localhost remote_user: root roles: - role: CachingOnDemand Finally run the playbook installation: ansible-playbook --extra-vars \"@xcache_config.yaml\" xcache_playbook.yaml Step by step deployment on bare metal: CMS XCache here","title":"Home"},{"location":"#on-demand-xcache-cluster","text":"","title":"On-Demand XCache cluster"},{"location":"#whats-xcache","text":"XCache description is available in this article here . You can look at the official XrootD documentation for detailed information about the XRootD tool: basic configuration cmsd configuration proxy file cache","title":"What's XCache"},{"location":"#xcache-components","text":"The setup infrastructure is shown the figure below, where the clients that run the payload can be instructed to request data to a cache system deployed on the same cloud provider and thus with low latency. The cache stack consists in: a proxy server to function as bridge between the private network of the cache and the client. This server will simply tunnel the request from cache servers. a cache redirector for federating each cache server deployed. If a new server is added, it will be automatically configured to contact this redirector for registration a configurable number of cache servers , the core of the tool that are responsibles for reading-ahead from remote site while caching. This setup has been tested on different cloud providers. It is also been tested at a scale of 2k concurrent jobs on Open Telekom Cloud resources in the context of HelixNebulaScience Cloud project. In the context of the eXtreme Data-Cloud project , a collection of recipes have been produced for the automatic deployment of a cache service on demand using different automation technology. For bare metal installation an Ansible playbook is available that can deploy either directly on host or through docker container the whole stack. For those who use docker swarm for container orchestration a docker-compose recipe is also available as for Kubernetes where an Helm chart is provided. All these solutions have been integrated in DODAS and thus with very few changes the same setup can be automatically replicated in different kind of cloud resources.","title":"XCache components"},{"location":"#authnz-mode-in-xcache","text":"The client show its identity only to the cache server The cache server will check in its local mapfile if the client is allowed to read the requested namespace If that is the case the cache server will server the file from its disk if already cached or it will use its own certificate (robot/service/power user as needed) to authenticate with the remote storage for the reading process The remote storage check its own mapfile if the robot/service/power user certificate is allowed to read from that namespace. N.B. a procedure to use a user proxy forwarding approach is available but not recomended for security reasons.","title":"AuthN/Z mode in XCache"},{"location":"#authnz-mode-in-xcache-with-oidc","text":"Coming soon...","title":"AuthN/Z mode in XCache with OIDC"},{"location":"#on-demand-xcache-deployment-with-docker-compose","text":"Please follow the instruction here","title":"On-demand XCache deployment with docker compose"},{"location":"#deployment-on-kubernetes-with-helm","text":"helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update helm install -n cache-cluster cloudpg/cachingondemand More details in this demo","title":"Deployment on Kubernetes with Helm"},{"location":"#deployment-with-dodas","text":"A guided demo is available here","title":"Deployment with DODAS"},{"location":"#ansible-deployment","text":"","title":"Ansible deployment"},{"location":"#requirements","text":"Ansible 2.4 OS: Centos7 valid CMS /etc/vomses Port: one open service port Valid grid host certifate Valid service certificate that is able to read from remote origin (to be stored in /etc/grid-security/xrd/xrdcert.pem, /etc/grid-security/xrd/xrdkey.pem)","title":"Requirements"},{"location":"#role-variables","text":"Create and customize your xcache ansible configuration (e.g. xcache_config.yaml ) xrootd_version: 4.8.3-1.el7 metricbeat_version: 6.2.4-x86_64 monitoring: # enable metricbeat sensors by setting this to \"metricbeat\" path_to_certs: /etc/grid-security/xrd BLOCK_SIZE: 512k # size of the file block used by the cache CACHE_LOG_LEVEL: info # server log level CACHE_PATH: /data/xrd # folder for cached files CACHE_RAM_GB: 12 # amount of RAM for caching in GB. Suggested ~50% of the total HI_WM: \"0.9\" # higher watermark of used fs space LOW_WM: \"0.8\" # lower watermark of used fs space N_PREFETCH: \"0\" # number of blocks to be prefetched ORIGIN_HOST: origin # hostname or ip adrr of the origin server ORIGIN_XRD_PORT: \"1094\" # xrootd port to contact origin on REDIR_HOST: xcache-service # hostname or ip adrr of the cache redirector REDIR_CMSD_PORT: \"31213\" # cmsd port of the cache redirector metricbeat_polltime: 60s # polling time of the metricbeat sensor metric_sitename: changeme # sitename to be displayed for monitoring elk_endpoint: localhost:9000 # elasticsearch endpoint url elastic_username: dodas # elasticsearch username elastic_password: testpass # elasticsearch password","title":"Role Variables"},{"location":"#example-playbook","text":"First install the role: ansible-galaxy install git+https://github.com/Cloud-PG/CachingOnDemand.git,ansible Then create a basic ansible playbook (e.g. xcache_playbook.yaml ) --- - hosts: localhost remote_user: root roles: - role: CachingOnDemand Finally run the playbook installation: ansible-playbook --extra-vars \"@xcache_config.yaml\" xcache_playbook.yaml","title":"Example Playbook"},{"location":"#step-by-step-deployment-on-bare-metal-cms-xcache","text":"here","title":"Step by step deployment on bare metal: CMS XCache"},{"location":"BARE/","text":"Step by step deployment on bare metal: CMS XCache Cache server standalone installation Requirements OS: Centos7 Port: one open service port Valid CMS /etc/vomses files Valid grid host certifate Valid service certificate that is able to read from AAA (/etc/grid-security/xrd/xrdcert.pem, /etc/grid-security/xrd/xrdkey.pem) Packages and CAs installation Create and execute the following script (we are going to install the 4.8.3 version for testing purpose, but consider also the 4.9 and 4.10 as they include new feature and fix): #!/bin/bash XRD_VERSION=4.8.3-1.el7 echo \"LC_ALL=C\" >> /etc/environment \\ && echo \"LANGUAGE=C\" >> /etc/environment \\ && yum --setopt=tsflags=nodocs -y update \\ && yum --setopt=tsflags=nodocs -y install wget \\ && yum clean all cd /etc/yum.repos.d wget http://repository.egi.eu/community/software/preview.repository/2.0/releases/repofiles/centos-7-x86_64.repo \\ && wget http://repository.egi.eu/sw/production/cas/1/current/repo-files/EGI-trustanchors.repo yum --setopt=tsflags=nodocs -y install epel-release yum-plugin-ovl \\ && yum --setopt=tsflags=nodocs -y install fetch-crl wn sysstat \\ && yum clean all yum install -y ca-policy-egi-core ca-policy-lcg /usr/sbin/fetch-crl -q yum install xrootd-server-$VERSION mkdir -p /etc/grid-security/xrd/ chown -R xrootd:xrootd /etc/grid-security/xrd/ systemctl enable fetch-crl-cron systemctl start fetch-crl-cron curl -L -O https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-6.2.4-x86_64.rpm sudo rpm -vi metricbeat-6.2.4-x86_64.rpm Proxy Renewal service In order to keep the service proxy valid a simple service che be created on the machine. First create a service file (e.g. /usr/lib/systemd/system/xrootd-renew-proxy.service ): [Unit] Description=Renew xrootd proxy [Service] User=xrootd Group=xrootd Type = oneshot ExecStart = /bin/voms-proxy-init --cert /etc/grid-security/xrd/cert/cert.pem --key /etc/grid-security/xrd/cert/key.pem -voms cms -valid 48:00 [Install] WantedBy=multi-user.target Then a timer service is required to manage the frequency of the proxy renewal ( /usr/lib/systemd/system/xrootd-renew-proxy.timer ): [Unit] Description=Renew proxy every day at midnight [Timer] OnCalendar=*-*-* 00:00:00 Unit=xrootd-renew-proxy.service [Install] WantedBy=multi-user.target At this point you can start and reload the services with: systemctl start xrootd-renew-proxy.timer systemctl daemon-reload XRootD server configuration The reference guide for the configuration is the official one here . For the version 4.9 or newer please refer to the recomended ones here What follows is a working point used and tested at different sites, its purpose is to show the main knobs available and how to threat them. Create a configuration file (e.g. /etc/xrootd/xrootd-xcache.cfg ) # xrd and cmsd process ports set xrdport=1094 set cmsdport=1213 # cache redirector address set rdtrCache=0.0.0.0 set rdtrPortCmsd=1213 # address and port of the origin servers set rdtrGlobal=xrootd-cms.infn.it set rdtrGlobalPort=1094 # disk occupation water marks set cacheLowWm=0.80 set cacheHiWm=0.90 # log level for cache processes set cacheLogLevel=info # path to folder for storing data, NB it has to be owned by xrootd user set cachePath=/data/xrd # ram dedicated to cache (in GB), <=50% of the total is suggested set cacheRam=16 all.manager $rdtrCache:$rdtrPortCmsd # logging level for all the different activities xrootd.trace info ofs.trace info xrd.trace info cms.trace info sec.trace info pfc.trace $cacheLogLevel if exec cmsd # if the process is the cluster manager, just run in on the chosen port all.role server xrd.port $cmsdport all.export / stage oss.localroot $cachePath else # if the process is the xrd one, configure and start the cache service at the specified port xrd.port $xrdport ##### GENERAL CONFIGURATION ###### # manage the work directory all.export / all.role server oss.localroot $cachePath oss.space meta $cachePath/ oss.space data $cachePath/ pfc.spaces data meta # in the system is overloaded fallback to remote read xrootd.fsoverload redirect xrootd-cms.infn.it:1094 # For xrootd, load the proxy plugin and the disk caching plugin. ofs.osslib libXrdPss.so pss.cachelib libXrdFileCache.so # indicate the origin pss.origin $rdtrGlobal:$rdtrGlobalPort ##### SECURITY CONFIGURATION ###### xrootd.seclib /usr/lib64/libXrdSec.so # use gsi as client-cache authN method sec.protocol /usr/lib64 gsi \\ -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrd/xrdcert.pem \\ -key:/etc/grid-security/xrd/xrdkey.pem \\ -d:3 \\ -crl:1 sec.protbind * gsi ofs.authorize 1 acc.audit deny grant # use gsi user<-->namespace mapping file as client-cache authZ method acc.authdb /etc/xrootd/Authfile-auth ##### CACHE CONFIGURATION ###### pfc.diskusage $cacheLowWm $cacheHiWm pfc.ram ${cacheRam}g # Tune the client timeouts to more aggressively timeout. pss.setopt ParallelEvtLoop 10 pss.setopt RequestTimeout 25 pss.setopt ConnectTimeout 25 pss.setopt ConnectionRetry 2 # Standard values for streaming mode jobs set cacheStreams=256 set prefetch=0 set blkSize=512k pss.config streams $cacheStreams pfc.blocksize $blkSize pfc.prefetch $prefetch fi In addition for the authZ part, a file has to be created with the desired permission per user (e.g. /etc/xrootd/Authfile-auth ): # full permissions to all users for both /store/* paths and /* u * /store/ a / a Start XCache deamons The only thing left now is to start the respective deamons with: # enable and start xrootd server deamons systemctl enable xrootd@xcache.service systemctl enable cmsd@xcache.service systemctl start xrootd@xcache.service systemctl start cmsd@xcache.service Test the deployment to check if the daemons started correctly just use systemctl as below: systemctl status xrootd@xcache.service systemctl status cmsd@xcache.service in case of problems logs can be found in /var/log/xrootd/xcache then you can try to copy a file from the origin: xrdcp -f -v xroot://localhost:<xrdport defined in the configuration above>/<path to your file in origin> the expected outcome is something like: [root@xrootdcentostest centos]# xrdcp -f -v xroot://localhost:32294//store/mc/RunIISummer17DRPremix/QCD_Pt-15to20_MuEnrichedPt5_TuneCUETP8M1_13TeV_pythia8/AODSIM/92X_upgrade2017_realistic_v10-v2/90000/C85940F6-9596-E711-8FD6-D8D385FF1940.root /dev/null [544MB/3.108GB][ 17%][========> ][19.43MB/s] Finally you should be able to see your file on the cache disk on the path you indicated in the configuration. Redirector installation The configuration for a cache redirector is really simple (e.g. /etc/xrootd/xrootd-cacheredir.cfg ) set rdtrcache=<redirector host> set rdtrportcmsd=<redirector cluster manager port> set rdtrportxrd=<redirector xrd port> all.manager $rdtrcache:$rdtrportcmsd # temporary fix for CMS multisource jobs - fixed probably by version 5 cms.sched maxretries 0 nomultisrc xrd.allow host * xrd.port $rdtrportxrd xrd.port $rdtrportcmsd if exec cmsd all.export /store stage r/o all.role manager and then just start the daemons: # enable and start xrootd redirector daemons systemctl enable xrootd@cacheredir.service systemctl enable cmsd@cacheredir.service systemctl start xrootd@cacheredir.service systemctl start cmsd@cacheredir.service Metricbeat installation Create a metricbeat configuration file (e.g. /etc/metricbeat/metricbeat.yml ): # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/metricbeat/index.html #========================== Modules configuration ============================ metricbeat.modules: #------------------------------- System Module ------------------------------- - module: system metricsets: # CPU stats - cpu # System Load stats - load # Per CPU core stats - core # IO stats - diskio # Per filesystem stats - filesystem # File system summary stats - fsstat # Memory stats - memory # Network stats - network # Per process stats - process # Sockets (linux only) #- socket enabled: true period: 60s processes: ['.*'] #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. name: 'DUMMY: cache sitename' #================================ Outputs ===================================== # Configure what outputs to use when sending the data collected by the beat. # Multiple outputs may be used. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"DUMMY_esHost.com\"] template.name: \"metricbeat_slave\" template.path: \"metricbeat.template.json\" template.overwrite: false # Optional protocol and basic auth credentials. protocol: \"http\" username: \"dodas\" password: \"DUMMY\" #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: critical, error, warning, info, debug #logging.level: debug and then just start the daemon: # enable and start metricbeat service systemctl enable metricbeat.service systemctl start metricbeat.service","title":"Quick Start with CMS XCache"},{"location":"BARE/#step-by-step-deployment-on-bare-metal-cms-xcache","text":"","title":"Step by step deployment on bare metal: CMS XCache"},{"location":"BARE/#cache-server-standalone-installation","text":"","title":"Cache server standalone installation"},{"location":"BARE/#requirements","text":"OS: Centos7 Port: one open service port Valid CMS /etc/vomses files Valid grid host certifate Valid service certificate that is able to read from AAA (/etc/grid-security/xrd/xrdcert.pem, /etc/grid-security/xrd/xrdkey.pem)","title":"Requirements"},{"location":"BARE/#packages-and-cas-installation","text":"Create and execute the following script (we are going to install the 4.8.3 version for testing purpose, but consider also the 4.9 and 4.10 as they include new feature and fix): #!/bin/bash XRD_VERSION=4.8.3-1.el7 echo \"LC_ALL=C\" >> /etc/environment \\ && echo \"LANGUAGE=C\" >> /etc/environment \\ && yum --setopt=tsflags=nodocs -y update \\ && yum --setopt=tsflags=nodocs -y install wget \\ && yum clean all cd /etc/yum.repos.d wget http://repository.egi.eu/community/software/preview.repository/2.0/releases/repofiles/centos-7-x86_64.repo \\ && wget http://repository.egi.eu/sw/production/cas/1/current/repo-files/EGI-trustanchors.repo yum --setopt=tsflags=nodocs -y install epel-release yum-plugin-ovl \\ && yum --setopt=tsflags=nodocs -y install fetch-crl wn sysstat \\ && yum clean all yum install -y ca-policy-egi-core ca-policy-lcg /usr/sbin/fetch-crl -q yum install xrootd-server-$VERSION mkdir -p /etc/grid-security/xrd/ chown -R xrootd:xrootd /etc/grid-security/xrd/ systemctl enable fetch-crl-cron systemctl start fetch-crl-cron curl -L -O https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-6.2.4-x86_64.rpm sudo rpm -vi metricbeat-6.2.4-x86_64.rpm","title":"Packages and CAs installation"},{"location":"BARE/#proxy-renewal-service","text":"In order to keep the service proxy valid a simple service che be created on the machine. First create a service file (e.g. /usr/lib/systemd/system/xrootd-renew-proxy.service ): [Unit] Description=Renew xrootd proxy [Service] User=xrootd Group=xrootd Type = oneshot ExecStart = /bin/voms-proxy-init --cert /etc/grid-security/xrd/cert/cert.pem --key /etc/grid-security/xrd/cert/key.pem -voms cms -valid 48:00 [Install] WantedBy=multi-user.target Then a timer service is required to manage the frequency of the proxy renewal ( /usr/lib/systemd/system/xrootd-renew-proxy.timer ): [Unit] Description=Renew proxy every day at midnight [Timer] OnCalendar=*-*-* 00:00:00 Unit=xrootd-renew-proxy.service [Install] WantedBy=multi-user.target At this point you can start and reload the services with: systemctl start xrootd-renew-proxy.timer systemctl daemon-reload","title":"Proxy Renewal service"},{"location":"BARE/#xrootd-server-configuration","text":"The reference guide for the configuration is the official one here . For the version 4.9 or newer please refer to the recomended ones here What follows is a working point used and tested at different sites, its purpose is to show the main knobs available and how to threat them. Create a configuration file (e.g. /etc/xrootd/xrootd-xcache.cfg ) # xrd and cmsd process ports set xrdport=1094 set cmsdport=1213 # cache redirector address set rdtrCache=0.0.0.0 set rdtrPortCmsd=1213 # address and port of the origin servers set rdtrGlobal=xrootd-cms.infn.it set rdtrGlobalPort=1094 # disk occupation water marks set cacheLowWm=0.80 set cacheHiWm=0.90 # log level for cache processes set cacheLogLevel=info # path to folder for storing data, NB it has to be owned by xrootd user set cachePath=/data/xrd # ram dedicated to cache (in GB), <=50% of the total is suggested set cacheRam=16 all.manager $rdtrCache:$rdtrPortCmsd # logging level for all the different activities xrootd.trace info ofs.trace info xrd.trace info cms.trace info sec.trace info pfc.trace $cacheLogLevel if exec cmsd # if the process is the cluster manager, just run in on the chosen port all.role server xrd.port $cmsdport all.export / stage oss.localroot $cachePath else # if the process is the xrd one, configure and start the cache service at the specified port xrd.port $xrdport ##### GENERAL CONFIGURATION ###### # manage the work directory all.export / all.role server oss.localroot $cachePath oss.space meta $cachePath/ oss.space data $cachePath/ pfc.spaces data meta # in the system is overloaded fallback to remote read xrootd.fsoverload redirect xrootd-cms.infn.it:1094 # For xrootd, load the proxy plugin and the disk caching plugin. ofs.osslib libXrdPss.so pss.cachelib libXrdFileCache.so # indicate the origin pss.origin $rdtrGlobal:$rdtrGlobalPort ##### SECURITY CONFIGURATION ###### xrootd.seclib /usr/lib64/libXrdSec.so # use gsi as client-cache authN method sec.protocol /usr/lib64 gsi \\ -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrd/xrdcert.pem \\ -key:/etc/grid-security/xrd/xrdkey.pem \\ -d:3 \\ -crl:1 sec.protbind * gsi ofs.authorize 1 acc.audit deny grant # use gsi user<-->namespace mapping file as client-cache authZ method acc.authdb /etc/xrootd/Authfile-auth ##### CACHE CONFIGURATION ###### pfc.diskusage $cacheLowWm $cacheHiWm pfc.ram ${cacheRam}g # Tune the client timeouts to more aggressively timeout. pss.setopt ParallelEvtLoop 10 pss.setopt RequestTimeout 25 pss.setopt ConnectTimeout 25 pss.setopt ConnectionRetry 2 # Standard values for streaming mode jobs set cacheStreams=256 set prefetch=0 set blkSize=512k pss.config streams $cacheStreams pfc.blocksize $blkSize pfc.prefetch $prefetch fi In addition for the authZ part, a file has to be created with the desired permission per user (e.g. /etc/xrootd/Authfile-auth ): # full permissions to all users for both /store/* paths and /* u * /store/ a / a","title":"XRootD server configuration"},{"location":"BARE/#start-xcache-deamons","text":"The only thing left now is to start the respective deamons with: # enable and start xrootd server deamons systemctl enable xrootd@xcache.service systemctl enable cmsd@xcache.service systemctl start xrootd@xcache.service systemctl start cmsd@xcache.service","title":"Start XCache deamons"},{"location":"BARE/#test-the-deployment","text":"to check if the daemons started correctly just use systemctl as below: systemctl status xrootd@xcache.service systemctl status cmsd@xcache.service in case of problems logs can be found in /var/log/xrootd/xcache then you can try to copy a file from the origin: xrdcp -f -v xroot://localhost:<xrdport defined in the configuration above>/<path to your file in origin> the expected outcome is something like: [root@xrootdcentostest centos]# xrdcp -f -v xroot://localhost:32294//store/mc/RunIISummer17DRPremix/QCD_Pt-15to20_MuEnrichedPt5_TuneCUETP8M1_13TeV_pythia8/AODSIM/92X_upgrade2017_realistic_v10-v2/90000/C85940F6-9596-E711-8FD6-D8D385FF1940.root /dev/null [544MB/3.108GB][ 17%][========> ][19.43MB/s] Finally you should be able to see your file on the cache disk on the path you indicated in the configuration.","title":"Test the deployment"},{"location":"BARE/#redirector-installation","text":"The configuration for a cache redirector is really simple (e.g. /etc/xrootd/xrootd-cacheredir.cfg ) set rdtrcache=<redirector host> set rdtrportcmsd=<redirector cluster manager port> set rdtrportxrd=<redirector xrd port> all.manager $rdtrcache:$rdtrportcmsd # temporary fix for CMS multisource jobs - fixed probably by version 5 cms.sched maxretries 0 nomultisrc xrd.allow host * xrd.port $rdtrportxrd xrd.port $rdtrportcmsd if exec cmsd all.export /store stage r/o all.role manager and then just start the daemons: # enable and start xrootd redirector daemons systemctl enable xrootd@cacheredir.service systemctl enable cmsd@cacheredir.service systemctl start xrootd@cacheredir.service systemctl start cmsd@cacheredir.service","title":"Redirector installation"},{"location":"BARE/#metricbeat-installation","text":"Create a metricbeat configuration file (e.g. /etc/metricbeat/metricbeat.yml ): # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/metricbeat/index.html #========================== Modules configuration ============================ metricbeat.modules: #------------------------------- System Module ------------------------------- - module: system metricsets: # CPU stats - cpu # System Load stats - load # Per CPU core stats - core # IO stats - diskio # Per filesystem stats - filesystem # File system summary stats - fsstat # Memory stats - memory # Network stats - network # Per process stats - process # Sockets (linux only) #- socket enabled: true period: 60s processes: ['.*'] #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. name: 'DUMMY: cache sitename' #================================ Outputs ===================================== # Configure what outputs to use when sending the data collected by the beat. # Multiple outputs may be used. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"DUMMY_esHost.com\"] template.name: \"metricbeat_slave\" template.path: \"metricbeat.template.json\" template.overwrite: false # Optional protocol and basic auth credentials. protocol: \"http\" username: \"dodas\" password: \"DUMMY\" #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: critical, error, warning, info, debug #logging.level: debug and then just start the daemon: # enable and start metricbeat service systemctl enable metricbeat.service systemctl start metricbeat.service","title":"Metricbeat installation"},{"location":"DOCKER/","text":"/home/dciangot/git/CachingOnDemand/docker/README.md","title":"Quick Start (Docker)"},{"location":"demo/DEMO/","text":"DEPLOY CACHINGONDEMAND STACK ON K8S Requirements VM with Vagrant If you have Vagrant installed or if you want to install it and to use it the following commands are enough to setup a working environment with everything you need for this demo: git clone https://github.com/Cloud-PG/CachingOnDemand cd CachingOnDemand vagrant up Manual installation For part 1: Docker Docker compose For part 2: k8s Docker in Docker kubectl and helm installed on the local machine remote xrootd host to connect to service certificate authorized to read from remote xrootd host kept on your local machine as user{cert,key}.pem voms configuration file (optional) kept on your local machine in folder ./vomses PART 1 Playground with Docker and Docker Compose Please follow the instruction here PART 2 Start a local Kubernetes cluster Let's start a k8s cluster locally with 4 nodes: NUM_NODES=3 k8s-dind up The output should be something like: * Bringing up coredns and kubernetes-dashboard deployment.extensions/coredns scaled deployment.extensions/kubernetes-dashboard scaled .............[done] NAME STATUS ROLES AGE VERSION kube-master Ready master 2m37s v1.13.0 kube-node-1 Ready <none> 116s v1.13.0 kube-node-2 Ready <none> 116s v1.13.0 kube-node-3 Ready <none> 116s v1.13.0 * Access dashboard at: http://127.0.0.1:32769/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy and dashboard should be accessible at the prompted link. Now your kube config file has been update so you should be able to query the cluster by: kubectl get node AuthN/Z mode in XCache The client show its identity only to the cache server The cache server will check in its local mapfile if the client is allowed to read the requested namespace If that is the case the cache server will server the file from its disk if already cached or it will use its own certificate (robot/service/power user as needed) to authenticate with the remote storage for the reading process The remote storage check its own mapfile if the robot/service/power user certificate is allowed to read from that namespace. N.B. a procedure to use a user proxy forwarding approach is available but not recomended for security reasons. Store certificates in K8S secrets Certificates can be saved on k8s and made available to all the cache server with this command: kubectl create secret generic certs --from-file=cert.pem=$PWD/usercert.pem --from-file=key.pem=$PWD/userkey.pem For more details about k8s secrets please visit this page Store vomses in configmap Vomses can be saved on k8s and made available to all the cache server with this command: kubectl create configmap vomses-config --from-file=vomses/ For more details about k8s configMaps please visit this page Install Helm and Cachingondemand repository Initialize helm on the cluster and then install the CachingOnDemand repository. helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update Taking a look to deployments parameters The deployment can be tune using the Helm following parameters (either passing them via yaml file or via CLI). cache: repository: cloudpg/cachingondemand tag: helm redir_host: xcache-redir-service.default.svc.cluster.local replicas: 1 cache_host_path: /data/xrd block_size: 512k mem_gb: 2 high_wm: 0.95 low_wm: 0.80 n_prefetch: 1 origin_host: cms-xrootd.infn.it origin_xrd_port: 1094 xrdport: 31494 cmsport: 31113 streams: 256 external_ip: 192.168.0.123 vo: \"\" http_port: 8080 redirector: repository: cloudpg/cachingondemand tag: helm replicas: 1 xrdport: 31294 cmsport: 31213 proxy: repository: cloudpg/cachingondemand tag: helm replicas: 1 external_ip: 192.168.0.123 xrdport: 31394 The default recipe will try to deploy the caching form in the following architecture. Deploy the cluster Let's use the default parameters for now (by default working with CMS remote end point): helm install -n cache-cluster cloudpg/cachingondemand the output of the command should look like: NAME: cache-demo LAST DEPLOYED: Fri May 31 14:57:02 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==> v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xcache-server-service NodePort 10.107.22.234 192.168.0.123 31494:31494/TCP,31113:31113/TCP 0s xcache-redir-service NodePort 10.110.216.224 192.168.0.123 31294:31294/TCP,31213:31213/TCP 0s xcache-proxy NodePort 10.108.121.41 192.168.0.123 31394:31394/TCP 0s ==> v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE xredir-pod 1 1 1 0 0s proxy-pod 1 1 1 0 0s xcache-pod 1 1 1 0 0s ==> v1/Pod(related) NAME READY STATUS RESTARTS AGE xredir-pod-8469496c75-mfwmq 0/1 ContainerCreating 0 0s proxy-pod-64c88f5c75-6p9md 0/1 ContainerCreating 0 0s xcache-pod-74b94865b4-tlmgb 0/2 ContainerCreating 0 0s Check the status If everthing went well, all the pods should be running after a while (varying with your internet connectivity): $> kubectl get pod NAME READY STATUS RESTARTS AGE proxy-pod-7ddbd957-rlndx 1/1 Running 0 16m xcache-pod-7c4dbd4667-9xhwt 3/3 Running 0 16m xredir-pod-696c4b764c-d7kjp 1/1 Running 0 16m Test the functionalities Now you can log into the client docker and test if you are able to download a file from the remote server through the cache server. kubectl exec -ti <xcache-pod ID> -c client bash export X509_USER_PROXY=/tmp/proxy xrdcp -v -d2 -f root://cache:32294//myfile.root /dev/null Customize the deployment: standalone cache server example The deployment of a standalone cache is possible passing the following values file (values.yaml): cache: redir_host: 0.0.0.0 redirector: replicas: 0 cmsport: 31113 proxy: replicas: 0 Then deploy the server with: helm install -n cache-standalone cloudpg/cachingondemand --values values.yaml","title":"Demo on K8s with Helm"},{"location":"demo/DEMO/#deploy-cachingondemand-stack-on-k8s","text":"","title":"DEPLOY CACHINGONDEMAND STACK ON K8S"},{"location":"demo/DEMO/#requirements","text":"","title":"Requirements"},{"location":"demo/DEMO/#vm-with-vagrant","text":"If you have Vagrant installed or if you want to install it and to use it the following commands are enough to setup a working environment with everything you need for this demo: git clone https://github.com/Cloud-PG/CachingOnDemand cd CachingOnDemand vagrant up","title":"VM with Vagrant"},{"location":"demo/DEMO/#manual-installation","text":"For part 1: Docker Docker compose For part 2: k8s Docker in Docker kubectl and helm installed on the local machine remote xrootd host to connect to service certificate authorized to read from remote xrootd host kept on your local machine as user{cert,key}.pem voms configuration file (optional) kept on your local machine in folder ./vomses","title":"Manual installation"},{"location":"demo/DEMO/#part-1","text":"","title":"PART 1"},{"location":"demo/DEMO/#playground-with-docker-and-docker-compose","text":"Please follow the instruction here","title":"Playground with Docker and Docker Compose"},{"location":"demo/DEMO/#part-2","text":"","title":"PART 2"},{"location":"demo/DEMO/#start-a-local-kubernetes-cluster","text":"Let's start a k8s cluster locally with 4 nodes: NUM_NODES=3 k8s-dind up The output should be something like: * Bringing up coredns and kubernetes-dashboard deployment.extensions/coredns scaled deployment.extensions/kubernetes-dashboard scaled .............[done] NAME STATUS ROLES AGE VERSION kube-master Ready master 2m37s v1.13.0 kube-node-1 Ready <none> 116s v1.13.0 kube-node-2 Ready <none> 116s v1.13.0 kube-node-3 Ready <none> 116s v1.13.0 * Access dashboard at: http://127.0.0.1:32769/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy and dashboard should be accessible at the prompted link. Now your kube config file has been update so you should be able to query the cluster by: kubectl get node","title":"Start a local Kubernetes cluster"},{"location":"demo/DEMO/#authnz-mode-in-xcache","text":"The client show its identity only to the cache server The cache server will check in its local mapfile if the client is allowed to read the requested namespace If that is the case the cache server will server the file from its disk if already cached or it will use its own certificate (robot/service/power user as needed) to authenticate with the remote storage for the reading process The remote storage check its own mapfile if the robot/service/power user certificate is allowed to read from that namespace. N.B. a procedure to use a user proxy forwarding approach is available but not recomended for security reasons.","title":"AuthN/Z mode in XCache"},{"location":"demo/DEMO/#store-certificates-in-k8s-secrets","text":"Certificates can be saved on k8s and made available to all the cache server with this command: kubectl create secret generic certs --from-file=cert.pem=$PWD/usercert.pem --from-file=key.pem=$PWD/userkey.pem For more details about k8s secrets please visit this page","title":"Store certificates in K8S secrets"},{"location":"demo/DEMO/#store-vomses-in-configmap","text":"Vomses can be saved on k8s and made available to all the cache server with this command: kubectl create configmap vomses-config --from-file=vomses/ For more details about k8s configMaps please visit this page","title":"Store vomses in configmap"},{"location":"demo/DEMO/#install-helm-and-cachingondemand-repository","text":"Initialize helm on the cluster and then install the CachingOnDemand repository. helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update","title":"Install Helm and Cachingondemand repository"},{"location":"demo/DEMO/#taking-a-look-to-deployments-parameters","text":"The deployment can be tune using the Helm following parameters (either passing them via yaml file or via CLI). cache: repository: cloudpg/cachingondemand tag: helm redir_host: xcache-redir-service.default.svc.cluster.local replicas: 1 cache_host_path: /data/xrd block_size: 512k mem_gb: 2 high_wm: 0.95 low_wm: 0.80 n_prefetch: 1 origin_host: cms-xrootd.infn.it origin_xrd_port: 1094 xrdport: 31494 cmsport: 31113 streams: 256 external_ip: 192.168.0.123 vo: \"\" http_port: 8080 redirector: repository: cloudpg/cachingondemand tag: helm replicas: 1 xrdport: 31294 cmsport: 31213 proxy: repository: cloudpg/cachingondemand tag: helm replicas: 1 external_ip: 192.168.0.123 xrdport: 31394 The default recipe will try to deploy the caching form in the following architecture.","title":"Taking a look to deployments parameters"},{"location":"demo/DEMO/#deploy-the-cluster","text":"Let's use the default parameters for now (by default working with CMS remote end point): helm install -n cache-cluster cloudpg/cachingondemand the output of the command should look like: NAME: cache-demo LAST DEPLOYED: Fri May 31 14:57:02 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==> v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xcache-server-service NodePort 10.107.22.234 192.168.0.123 31494:31494/TCP,31113:31113/TCP 0s xcache-redir-service NodePort 10.110.216.224 192.168.0.123 31294:31294/TCP,31213:31213/TCP 0s xcache-proxy NodePort 10.108.121.41 192.168.0.123 31394:31394/TCP 0s ==> v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE xredir-pod 1 1 1 0 0s proxy-pod 1 1 1 0 0s xcache-pod 1 1 1 0 0s ==> v1/Pod(related) NAME READY STATUS RESTARTS AGE xredir-pod-8469496c75-mfwmq 0/1 ContainerCreating 0 0s proxy-pod-64c88f5c75-6p9md 0/1 ContainerCreating 0 0s xcache-pod-74b94865b4-tlmgb 0/2 ContainerCreating 0 0s","title":"Deploy the cluster"},{"location":"demo/DEMO/#check-the-status","text":"If everthing went well, all the pods should be running after a while (varying with your internet connectivity): $> kubectl get pod NAME READY STATUS RESTARTS AGE proxy-pod-7ddbd957-rlndx 1/1 Running 0 16m xcache-pod-7c4dbd4667-9xhwt 3/3 Running 0 16m xredir-pod-696c4b764c-d7kjp 1/1 Running 0 16m","title":"Check the status"},{"location":"demo/DEMO/#test-the-functionalities","text":"Now you can log into the client docker and test if you are able to download a file from the remote server through the cache server. kubectl exec -ti <xcache-pod ID> -c client bash export X509_USER_PROXY=/tmp/proxy xrdcp -v -d2 -f root://cache:32294//myfile.root /dev/null","title":"Test the functionalities"},{"location":"demo/DEMO/#customize-the-deployment-standalone-cache-server-example","text":"The deployment of a standalone cache is possible passing the following values file (values.yaml): cache: redir_host: 0.0.0.0 redirector: replicas: 0 cmsport: 31113 proxy: replicas: 0 Then deploy the server with: helm install -n cache-standalone cloudpg/cachingondemand --values values.yaml","title":"Customize the deployment: standalone cache server example"},{"location":"demo/DODAS/","text":"DEPLOY CACHINGONDEMAND STACK WITH DODAS What's DODAS Please take a look at this article Components The following components will be intalled with the procedure presented below: XCache server : GSI authentication for both client and remote storage Cache federator : allowing dynamic cache server scaling Proxy server : providing a tunnel between external client and cache servers on internal network. Prometheus server and node exporters : providing basic knobs for a cluster monitoring Grafana server : for visualization of dashboard starting from data stored by prometheus IM client and DODAS template For a introduction and a quickstart guide for DODAS please see the documentation page . In this demo we will make use of the IM python client . As first step make sure to clone the TOSCA template repository : git clone https://github.com/indigo-dc/tosca-templates cd tosca-templates/dodas git checkout k8s_cms Now this kind of deployment will automatize 2 macro steps: creation of a K8s cluster with Helm installing with Helm the needed applications configuring this application using a yaml file provided by the user So let's take a look to the main feature of the TOSCA template and Helm configuration files. TOSCA template Available on github CachingOnDemand Helm values Available on github Prometheus Helm values Available on github Grafana Helm values Available on github Create a cluster with IM client wget https://github.com/indigo-dc/tosca-templates/tree/k8s_cms/dodas/XCache-demo.yaml im_client.py -a my_auth_file.dat create XCache-demo.yaml Create K8s secrets and configmaps First log into the k8s master machine and copy there your certificates wherever you prefer e.g. Test functionalities Monitor system and dummy grafana dashboard ... Future improvements Manage K8s VolumeClaims for a dynamic and flexible storage management Make possible to use the K8s ConfingMap for passing custom server configuration","title":"Demo deployment with DODAS"},{"location":"demo/DODAS/#deploy-cachingondemand-stack-with-dodas","text":"","title":"DEPLOY CACHINGONDEMAND STACK WITH DODAS"},{"location":"demo/DODAS/#whats-dodas","text":"Please take a look at this article","title":"What's DODAS"},{"location":"demo/DODAS/#components","text":"The following components will be intalled with the procedure presented below: XCache server : GSI authentication for both client and remote storage Cache federator : allowing dynamic cache server scaling Proxy server : providing a tunnel between external client and cache servers on internal network. Prometheus server and node exporters : providing basic knobs for a cluster monitoring Grafana server : for visualization of dashboard starting from data stored by prometheus","title":"Components"},{"location":"demo/DODAS/#im-client-and-dodas-template","text":"For a introduction and a quickstart guide for DODAS please see the documentation page . In this demo we will make use of the IM python client . As first step make sure to clone the TOSCA template repository : git clone https://github.com/indigo-dc/tosca-templates cd tosca-templates/dodas git checkout k8s_cms Now this kind of deployment will automatize 2 macro steps: creation of a K8s cluster with Helm installing with Helm the needed applications configuring this application using a yaml file provided by the user So let's take a look to the main feature of the TOSCA template and Helm configuration files.","title":"IM client and DODAS template"},{"location":"demo/DODAS/#tosca-template","text":"Available on github","title":"TOSCA template"},{"location":"demo/DODAS/#cachingondemand-helm-values","text":"Available on github","title":"CachingOnDemand Helm values"},{"location":"demo/DODAS/#prometheus-helm-values","text":"Available on github","title":"Prometheus Helm values"},{"location":"demo/DODAS/#grafana-helm-values","text":"Available on github","title":"Grafana Helm values"},{"location":"demo/DODAS/#create-a-cluster-with-im-client","text":"wget https://github.com/indigo-dc/tosca-templates/tree/k8s_cms/dodas/XCache-demo.yaml im_client.py -a my_auth_file.dat create XCache-demo.yaml","title":"Create a cluster with IM client"},{"location":"demo/DODAS/#create-k8s-secrets-and-configmaps","text":"First log into the k8s master machine and copy there your certificates wherever you prefer e.g.","title":"Create K8s secrets and configmaps"},{"location":"demo/DODAS/#test-functionalities","text":"","title":"Test functionalities"},{"location":"demo/DODAS/#monitor-system-and-dummy-grafana-dashboard","text":"...","title":"Monitor system and dummy grafana dashboard"},{"location":"demo/DODAS/#future-improvements","text":"Manage K8s VolumeClaims for a dynamic and flexible storage management Make possible to use the K8s ConfingMap for passing custom server configuration","title":"Future improvements"}]}