{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"On-Demand XCache cluster What's XCache XCache description is available in this article here . You can look at the official XrootD documentation for detailed information about the XRootD tool: basic configuration cmsd configuration proxy file cache XCache components The setup infrastructure is shown the figure below, where the clients that run the payload can be instructed to request data to a cache system deployed on the same cloud provider and thus with low latency. The cache stack consists in: a proxy server to function as bridge between the private network of the cache and the client. This server will simply tunnel the request from cache servers. a cache redirector for federating each cache server deployed. If a new server is added, it will be automatically configured to contact this redirector for registration a configurable number of cache servers , the core of the tool that are responsibles for reading-ahead from remote site while caching. This setup has been tested on different cloud providers. It is also been tested at a scale of 2k concurrent jobs on Open Telekom Cloud resources in the context of HelixNebulaScience Cloud project. In the context of the eXtreme Data-Cloud project , a collection of recipes have been produced for the automatic deployment of a cache service on demand using different automation technology. For bare metal installation an Ansible playbook is available that can deploy either directly on host or through docker container the whole stack. For those who use docker swarm for container orchestration a docker-compose recipe is also available as for Kubernetes where an Helm chart is provided. All these solutions have been integrated in DODAS and thus with very few changes the same setup can be automatically replicated in different kind of cloud resources. AuthN/Z mode in XCache The client show its identity only to the cache server The cache server will check in its local mapfile if the client is allowed to read the requested namespace If that is the case the cache server will server the file from its disk if already cached or it will use its own certificate (robot/service/power user as needed) to authenticate with the remote storage for the reading process The remote storage check its own mapfile if the robot/service/power user certificate is allowed to read from that namespace. N.B. a procedure to use a user proxy forwarding approach is available but not recomended for security reasons. AuthN/Z mode in XCache with OIDC Coming soon... On-demand XCache deployment with docker compose Please follow the instruction here Deployment on Kubernetes with Helm helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update helm install -n cache-cluster cloudpg/cachingondemand More details in this demo Deployment with DODAS A guided demo is available here Ansible deployment Requirements Ansible 2.4 OS: Centos7 valid CMS /etc/vomses Port: one open service port Valid grid host certifate Valid service certificate that is able to read from remote origin (to be stored in /etc/grid-security/xrd/xrdcert.pem, /etc/grid-security/xrd/xrdkey.pem) Role Variables BLOCK_SIZE: 512k # size of the file block used by the cache CACHE_LOG_LEVEL: info # server log level CACHE_PATH: /data/xrd # folder for cached files CACHE_RAM_GB: 12 # amount of RAM for caching in GB. Suggested ~50% of the total HI_WM: 0.9 # higher watermark of used fs space LOW_WM: 0.8 # lower watermark of used fs space N_PREFETCH: 0 # number of blocks to be prefetched ORIGIN_HOST: origin # hostname or ip adrr of the origin server ORIGIN_XRD_PORT: 1094 # xrootd port to contact origin on REDIR_HOST: xcache-service # hostname or ip adrr of the cache redirector REDIR_CMSD_PORT: 31213 # cmsd port of the cache redirector metricbeat_polltime: 60s # polling time of the metricbeat sensor metric_sitename: changeme # sitename to be displayed for monitoring elk_endpoint: localhost:9000 # elasticsearch endpoint url elastic_username: dodas # elasticsearch username elastic_password: testpass # elasticsearch password Example Playbook --- - hosts: localhost remote_user: root roles: - role: cloudpg.cachingondemand Deployment example: CMS XCache https://xcache.readthedocs.io/en/latest/automated-grid.html","title":"Home"},{"location":"#on-demand-xcache-cluster","text":"","title":"On-Demand XCache cluster"},{"location":"#whats-xcache","text":"XCache description is available in this article here . You can look at the official XrootD documentation for detailed information about the XRootD tool: basic configuration cmsd configuration proxy file cache","title":"What's XCache"},{"location":"#xcache-components","text":"The setup infrastructure is shown the figure below, where the clients that run the payload can be instructed to request data to a cache system deployed on the same cloud provider and thus with low latency. The cache stack consists in: a proxy server to function as bridge between the private network of the cache and the client. This server will simply tunnel the request from cache servers. a cache redirector for federating each cache server deployed. If a new server is added, it will be automatically configured to contact this redirector for registration a configurable number of cache servers , the core of the tool that are responsibles for reading-ahead from remote site while caching. This setup has been tested on different cloud providers. It is also been tested at a scale of 2k concurrent jobs on Open Telekom Cloud resources in the context of HelixNebulaScience Cloud project. In the context of the eXtreme Data-Cloud project , a collection of recipes have been produced for the automatic deployment of a cache service on demand using different automation technology. For bare metal installation an Ansible playbook is available that can deploy either directly on host or through docker container the whole stack. For those who use docker swarm for container orchestration a docker-compose recipe is also available as for Kubernetes where an Helm chart is provided. All these solutions have been integrated in DODAS and thus with very few changes the same setup can be automatically replicated in different kind of cloud resources.","title":"XCache components"},{"location":"#authnz-mode-in-xcache","text":"The client show its identity only to the cache server The cache server will check in its local mapfile if the client is allowed to read the requested namespace If that is the case the cache server will server the file from its disk if already cached or it will use its own certificate (robot/service/power user as needed) to authenticate with the remote storage for the reading process The remote storage check its own mapfile if the robot/service/power user certificate is allowed to read from that namespace. N.B. a procedure to use a user proxy forwarding approach is available but not recomended for security reasons.","title":"AuthN/Z mode in XCache"},{"location":"#authnz-mode-in-xcache-with-oidc","text":"Coming soon...","title":"AuthN/Z mode in XCache with OIDC"},{"location":"#on-demand-xcache-deployment-with-docker-compose","text":"Please follow the instruction here","title":"On-demand XCache deployment with docker compose"},{"location":"#deployment-on-kubernetes-with-helm","text":"helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update helm install -n cache-cluster cloudpg/cachingondemand More details in this demo","title":"Deployment on Kubernetes with Helm"},{"location":"#deployment-with-dodas","text":"A guided demo is available here","title":"Deployment with DODAS"},{"location":"#ansible-deployment","text":"","title":"Ansible deployment"},{"location":"#requirements","text":"Ansible 2.4 OS: Centos7 valid CMS /etc/vomses Port: one open service port Valid grid host certifate Valid service certificate that is able to read from remote origin (to be stored in /etc/grid-security/xrd/xrdcert.pem, /etc/grid-security/xrd/xrdkey.pem)","title":"Requirements"},{"location":"#role-variables","text":"BLOCK_SIZE: 512k # size of the file block used by the cache CACHE_LOG_LEVEL: info # server log level CACHE_PATH: /data/xrd # folder for cached files CACHE_RAM_GB: 12 # amount of RAM for caching in GB. Suggested ~50% of the total HI_WM: 0.9 # higher watermark of used fs space LOW_WM: 0.8 # lower watermark of used fs space N_PREFETCH: 0 # number of blocks to be prefetched ORIGIN_HOST: origin # hostname or ip adrr of the origin server ORIGIN_XRD_PORT: 1094 # xrootd port to contact origin on REDIR_HOST: xcache-service # hostname or ip adrr of the cache redirector REDIR_CMSD_PORT: 31213 # cmsd port of the cache redirector metricbeat_polltime: 60s # polling time of the metricbeat sensor metric_sitename: changeme # sitename to be displayed for monitoring elk_endpoint: localhost:9000 # elasticsearch endpoint url elastic_username: dodas # elasticsearch username elastic_password: testpass # elasticsearch password","title":"Role Variables"},{"location":"#example-playbook","text":"--- - hosts: localhost remote_user: root roles: - role: cloudpg.cachingondemand","title":"Example Playbook"},{"location":"#deployment-example-cms-xcache","text":"https://xcache.readthedocs.io/en/latest/automated-grid.html","title":"Deployment example: CMS XCache"},{"location":"DOCKER/","text":"USAGE Available options --nogsi : avoid client server gsi auth --nogrid : avoid WLCG CAs installation --health_port : port for healthcheck process listening, type=int, default=80 --config : specify xrootd config file Important container paths /data/xrd/ : saved file location for both cache and std modes /etc/xrootd/ : config files dir /var/log/xrootd/cmsd.log : log of cmsd service /var/log/xrootd/xrootd.log : log of xrootd service Running by hand Just put your xrootd config file in $PWD/config:/etc/xrootd # with your xrd_cache.conf on $PWD/config sudo docker run --rm --privileged -p 32294:32294 -p 31113:31113 -v $PWD/config:/etc/xrootd cloudpg/cachingondemand --config /etc/xrootd/xrd_test.conf REMEMBER To expose the ports indicated in your config file. In the case of config/xrd_test.conf are: 32294, 31113 File saved will be put on /data/xrd, so you may want to mount your storage backend there An health check is available on: # response 0 everything running, response 1 something went wrong curl container_ip /check_health In case of response 1, with docker logs you can see a log dump of the crashed daemon. Local stack deployment with docker compose: Origin+Cache+CacheRedirector You need to first install docker-compose . Then run use the docker-compose.yml file provided to bring up locally: an origin server with config in config/xrd_test_origin.conf a file cache server with config in config/xrd_test.conf a file cache redirector with config in config/xrd_test-redir.conf a portainer webUI for quickly debug and move throught the containers (available on localhost:9000 once deployed) The command for bringing the full stack up is: git clone https://github.com/Cloud-PG/cachingondemand.git cd cachingondemand/docker /usr/local/bin/docker-compose up -d To shutdown the stack: /usr/local/bin/docker-compose down First functional test # Put a test file on the remote host sudo docker exec -ti docker_client_1 sh -c echo \\ This is my file\\ test.txt xrdcp test.txt root://docker_origin_1:1194//test.txt # Request that file from the cache redirector xrootd process # that is listening on 1094 sudo docker exec -ti docker_client_1 xrdcp -f -d3 root://docker_redirector_1//test.txt remote_test.txt # If you find no error you can now check that the file is correctly cached on cache server sudo docker exec -ti docker_cache_1 ls /data/xrd/ # you should see these two files: test.txt test.txt.cinfo","title":"Quick Start (Docker)"},{"location":"DOCKER/#usage","text":"","title":"USAGE"},{"location":"DOCKER/#available-options","text":"--nogsi : avoid client server gsi auth --nogrid : avoid WLCG CAs installation --health_port : port for healthcheck process listening, type=int, default=80 --config : specify xrootd config file","title":"Available options"},{"location":"DOCKER/#important-container-paths","text":"/data/xrd/ : saved file location for both cache and std modes /etc/xrootd/ : config files dir /var/log/xrootd/cmsd.log : log of cmsd service /var/log/xrootd/xrootd.log : log of xrootd service","title":"Important container paths"},{"location":"DOCKER/#running-by-hand","text":"Just put your xrootd config file in $PWD/config:/etc/xrootd # with your xrd_cache.conf on $PWD/config sudo docker run --rm --privileged -p 32294:32294 -p 31113:31113 -v $PWD/config:/etc/xrootd cloudpg/cachingondemand --config /etc/xrootd/xrd_test.conf REMEMBER To expose the ports indicated in your config file. In the case of config/xrd_test.conf are: 32294, 31113 File saved will be put on /data/xrd, so you may want to mount your storage backend there An health check is available on: # response 0 everything running, response 1 something went wrong curl container_ip /check_health In case of response 1, with docker logs you can see a log dump of the crashed daemon.","title":"Running by hand"},{"location":"DOCKER/#local-stack-deployment-with-docker-compose-origincachecacheredirector","text":"You need to first install docker-compose . Then run use the docker-compose.yml file provided to bring up locally: an origin server with config in config/xrd_test_origin.conf a file cache server with config in config/xrd_test.conf a file cache redirector with config in config/xrd_test-redir.conf a portainer webUI for quickly debug and move throught the containers (available on localhost:9000 once deployed) The command for bringing the full stack up is: git clone https://github.com/Cloud-PG/cachingondemand.git cd cachingondemand/docker /usr/local/bin/docker-compose up -d To shutdown the stack: /usr/local/bin/docker-compose down","title":"Local stack deployment with docker compose: Origin+Cache+CacheRedirector"},{"location":"DOCKER/#first-functional-test","text":"# Put a test file on the remote host sudo docker exec -ti docker_client_1 sh -c echo \\ This is my file\\ test.txt xrdcp test.txt root://docker_origin_1:1194//test.txt # Request that file from the cache redirector xrootd process # that is listening on 1094 sudo docker exec -ti docker_client_1 xrdcp -f -d3 root://docker_redirector_1//test.txt remote_test.txt # If you find no error you can now check that the file is correctly cached on cache server sudo docker exec -ti docker_cache_1 ls /data/xrd/ # you should see these two files: test.txt test.txt.cinfo","title":"First functional test"},{"location":"demo/DEMO/","text":"DEPLOY CACHINGONDEMAND STACK ON K8S Requirements For part 1: Docker Docker compose For part 2: k8s Docker in Docker kubectl and helm installed on the local machine remote xrootd host to connect to service certificate authorized to read from remote xrootd host kept on your local machine as user{cert,key}.pem voms configuration file (optional) kept on your local machine in folder ./vomses PART 1 Playground with Docker and Docker Compose Please follow the instruction here PART 2 Start a local Kubernetes cluster Let's start a k8s cluster locally with 4 nodes: NUM_NODES=3 k8s-dind up The output should be something like: * Bringing up coredns and kubernetes-dashboard deployment.extensions/coredns scaled deployment.extensions/kubernetes-dashboard scaled .............[done] NAME STATUS ROLES AGE VERSION kube-master Ready master 2m37s v1.13.0 kube-node-1 Ready none 116s v1.13.0 kube-node-2 Ready none 116s v1.13.0 kube-node-3 Ready none 116s v1.13.0 * Access dashboard at: http://127.0.0.1:32769/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy and dashboard should be accessible at the prompted link. Now your kube config file has been update so you should be able to query the cluster by: kubectl get node AuthN/Z mode in XCache The client show its identity only to the cache server The cache server will check in its local mapfile if the client is allowed to read the requested namespace If that is the case the cache server will server the file from its disk if already cached or it will use its own certificate (robot/service/power user as needed) to authenticate with the remote storage for the reading process The remote storage check its own mapfile if the robot/service/power user certificate is allowed to read from that namespace. N.B. a procedure to use a user proxy forwarding approach is available but not recomended for security reasons. Store certificates in K8S secrets Certificates can be saved on k8s and made available to all the cache server with this command: kubectl create secret generic certs --from-file=cert.pem=$PWD/usercert.pem --from-file=key.pem=$PWD/userkey.pem For more details about k8s secrets please visit this page Store vomses in configmap Vomses can be saved on k8s and made available to all the cache server with this command: kubectl create configmap vomses-config --from-file=vomses/ For more details about k8s configMaps please visit this page Install Helm and Cachingondemand repository Initialize helm on the cluster and then install the CachingOnDemand repository. helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update Taking a look to deployments parameters The deployment can be tune using the Helm following parameters (either passing them via yaml file or via CLI). cache: repository: cloudpg/cachingondemand tag: helm redir_host: xcache-redir-service.default.svc.cluster.local replicas: 1 cache_host_path: /data/xrd block_size: 512k mem_gb: 2 high_wm: 0.95 low_wm: 0.80 n_prefetch: 1 origin_host: cms-xrootd.infn.it origin_xrd_port: 1094 xrdport: 31494 cmsport: 31113 streams: 256 external_ip: 192.168.0.123 vo: http_port: 8080 redirector: repository: cloudpg/cachingondemand tag: helm replicas: 1 xrdport: 31294 cmsport: 31213 proxy: repository: cloudpg/cachingondemand tag: helm replicas: 1 external_ip: 192.168.0.123 xrdport: 31394 The default recipe will try to deploy the caching form in the following architecture. Deploy the cluster Let's use the default parameters for now (by default working with CMS remote end point): helm install -n cache-cluster cloudpg/cachingondemand the output of the command should look like: NAME: cache-demo LAST DEPLOYED: Fri May 31 14:57:02 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: == v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xcache-server-service NodePort 10.107.22.234 192.168.0.123 31494:31494/TCP,31113:31113/TCP 0s xcache-redir-service NodePort 10.110.216.224 192.168.0.123 31294:31294/TCP,31213:31213/TCP 0s xcache-proxy NodePort 10.108.121.41 192.168.0.123 31394:31394/TCP 0s == v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE xredir-pod 1 1 1 0 0s proxy-pod 1 1 1 0 0s xcache-pod 1 1 1 0 0s == v1/Pod(related) NAME READY STATUS RESTARTS AGE xredir-pod-8469496c75-mfwmq 0/1 ContainerCreating 0 0s proxy-pod-64c88f5c75-6p9md 0/1 ContainerCreating 0 0s xcache-pod-74b94865b4-tlmgb 0/2 ContainerCreating 0 0s Check the status Test the functionalities Customize the deployment: standalone cache server example","title":"Demo on K8s with Heml"},{"location":"demo/DEMO/#deploy-cachingondemand-stack-on-k8s","text":"","title":"DEPLOY CACHINGONDEMAND STACK ON K8S"},{"location":"demo/DEMO/#requirements","text":"For part 1: Docker Docker compose For part 2: k8s Docker in Docker kubectl and helm installed on the local machine remote xrootd host to connect to service certificate authorized to read from remote xrootd host kept on your local machine as user{cert,key}.pem voms configuration file (optional) kept on your local machine in folder ./vomses","title":"Requirements"},{"location":"demo/DEMO/#part-1","text":"","title":"PART 1"},{"location":"demo/DEMO/#playground-with-docker-and-docker-compose","text":"Please follow the instruction here","title":"Playground with Docker and Docker Compose"},{"location":"demo/DEMO/#part-2","text":"","title":"PART 2"},{"location":"demo/DEMO/#start-a-local-kubernetes-cluster","text":"Let's start a k8s cluster locally with 4 nodes: NUM_NODES=3 k8s-dind up The output should be something like: * Bringing up coredns and kubernetes-dashboard deployment.extensions/coredns scaled deployment.extensions/kubernetes-dashboard scaled .............[done] NAME STATUS ROLES AGE VERSION kube-master Ready master 2m37s v1.13.0 kube-node-1 Ready none 116s v1.13.0 kube-node-2 Ready none 116s v1.13.0 kube-node-3 Ready none 116s v1.13.0 * Access dashboard at: http://127.0.0.1:32769/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy and dashboard should be accessible at the prompted link. Now your kube config file has been update so you should be able to query the cluster by: kubectl get node","title":"Start a local Kubernetes cluster"},{"location":"demo/DEMO/#authnz-mode-in-xcache","text":"The client show its identity only to the cache server The cache server will check in its local mapfile if the client is allowed to read the requested namespace If that is the case the cache server will server the file from its disk if already cached or it will use its own certificate (robot/service/power user as needed) to authenticate with the remote storage for the reading process The remote storage check its own mapfile if the robot/service/power user certificate is allowed to read from that namespace. N.B. a procedure to use a user proxy forwarding approach is available but not recomended for security reasons.","title":"AuthN/Z mode in XCache"},{"location":"demo/DEMO/#store-certificates-in-k8s-secrets","text":"Certificates can be saved on k8s and made available to all the cache server with this command: kubectl create secret generic certs --from-file=cert.pem=$PWD/usercert.pem --from-file=key.pem=$PWD/userkey.pem For more details about k8s secrets please visit this page","title":"Store certificates in K8S secrets"},{"location":"demo/DEMO/#store-vomses-in-configmap","text":"Vomses can be saved on k8s and made available to all the cache server with this command: kubectl create configmap vomses-config --from-file=vomses/ For more details about k8s configMaps please visit this page","title":"Store vomses in configmap"},{"location":"demo/DEMO/#install-helm-and-cachingondemand-repository","text":"Initialize helm on the cluster and then install the CachingOnDemand repository. helm init --upgrade helm repo add cloudpg https://cloud-pg.github.io/CachingOnDemand/ helm repo update","title":"Install Helm and Cachingondemand repository"},{"location":"demo/DEMO/#taking-a-look-to-deployments-parameters","text":"The deployment can be tune using the Helm following parameters (either passing them via yaml file or via CLI). cache: repository: cloudpg/cachingondemand tag: helm redir_host: xcache-redir-service.default.svc.cluster.local replicas: 1 cache_host_path: /data/xrd block_size: 512k mem_gb: 2 high_wm: 0.95 low_wm: 0.80 n_prefetch: 1 origin_host: cms-xrootd.infn.it origin_xrd_port: 1094 xrdport: 31494 cmsport: 31113 streams: 256 external_ip: 192.168.0.123 vo: http_port: 8080 redirector: repository: cloudpg/cachingondemand tag: helm replicas: 1 xrdport: 31294 cmsport: 31213 proxy: repository: cloudpg/cachingondemand tag: helm replicas: 1 external_ip: 192.168.0.123 xrdport: 31394 The default recipe will try to deploy the caching form in the following architecture.","title":"Taking a look to deployments parameters"},{"location":"demo/DEMO/#deploy-the-cluster","text":"Let's use the default parameters for now (by default working with CMS remote end point): helm install -n cache-cluster cloudpg/cachingondemand the output of the command should look like: NAME: cache-demo LAST DEPLOYED: Fri May 31 14:57:02 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: == v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xcache-server-service NodePort 10.107.22.234 192.168.0.123 31494:31494/TCP,31113:31113/TCP 0s xcache-redir-service NodePort 10.110.216.224 192.168.0.123 31294:31294/TCP,31213:31213/TCP 0s xcache-proxy NodePort 10.108.121.41 192.168.0.123 31394:31394/TCP 0s == v1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE xredir-pod 1 1 1 0 0s proxy-pod 1 1 1 0 0s xcache-pod 1 1 1 0 0s == v1/Pod(related) NAME READY STATUS RESTARTS AGE xredir-pod-8469496c75-mfwmq 0/1 ContainerCreating 0 0s proxy-pod-64c88f5c75-6p9md 0/1 ContainerCreating 0 0s xcache-pod-74b94865b4-tlmgb 0/2 ContainerCreating 0 0s","title":"Deploy the cluster"},{"location":"demo/DEMO/#check-the-status","text":"","title":"Check the status"},{"location":"demo/DEMO/#test-the-functionalities","text":"","title":"Test the functionalities"},{"location":"demo/DEMO/#customize-the-deployment-standalone-cache-server-example","text":"","title":"Customize the deployment: standalone cache server example"},{"location":"demo/DODAS/","text":"DEPLOY CACHINGONDEMAND STACK WITH DODAS What's DODAS Please take a look at this article Components The following components will be intalled with the procedure presented below: XCache server : GSI authentication for both client and remote storage Cache federator : allowing dynamic cache server scaling Proxy server : providing a tunnel between external client and cache servers on internal network. Prometheus server and node exporters : providing basic knobs for a cluster monitoring Grafana server : for visualization of dashboard starting from data stored by prometheus IM client and DODAS template For a introduction and a quickstart guide for DODAS please see the documentation page . In this demo we will make use of the IM python client . As first step make sure to clone the TOSCA template repository : git clone https://github.com/indigo-dc/tosca-templates cd tosca-templates/dodas git checkout k8s_cms Now this kind of deployment will automatize 2 macro steps: creation of a K8s cluster with Helm installing with Helm the needed applications configuring this application using a yaml file provided by the user So let's take a look to the main feature of the TOSCA template and Helm configuration files. TOSCA template Available on github CachingOnDemand Helm values Available on github Prometheus Helm values Available on github Grafana Helm values Available on github Create a cluster with IM client wget https://github.com/indigo-dc/tosca-templates/tree/k8s_cms/dodas/XCache-demo.yaml im_client.py -a my_auth_file.dat create XCache-demo.yaml Create K8s secrets and configmaps First log into the k8s master machine and copy there your certificates wherever you prefer e.g. Test functionalities Monitor system and dummy grafana dashboard Future improvements","title":"Demo deployment with DODAS"},{"location":"demo/DODAS/#deploy-cachingondemand-stack-with-dodas","text":"","title":"DEPLOY CACHINGONDEMAND STACK WITH DODAS"},{"location":"demo/DODAS/#whats-dodas","text":"Please take a look at this article","title":"What's DODAS"},{"location":"demo/DODAS/#components","text":"The following components will be intalled with the procedure presented below: XCache server : GSI authentication for both client and remote storage Cache federator : allowing dynamic cache server scaling Proxy server : providing a tunnel between external client and cache servers on internal network. Prometheus server and node exporters : providing basic knobs for a cluster monitoring Grafana server : for visualization of dashboard starting from data stored by prometheus","title":"Components"},{"location":"demo/DODAS/#im-client-and-dodas-template","text":"For a introduction and a quickstart guide for DODAS please see the documentation page . In this demo we will make use of the IM python client . As first step make sure to clone the TOSCA template repository : git clone https://github.com/indigo-dc/tosca-templates cd tosca-templates/dodas git checkout k8s_cms Now this kind of deployment will automatize 2 macro steps: creation of a K8s cluster with Helm installing with Helm the needed applications configuring this application using a yaml file provided by the user So let's take a look to the main feature of the TOSCA template and Helm configuration files.","title":"IM client and DODAS template"},{"location":"demo/DODAS/#tosca-template","text":"Available on github","title":"TOSCA template"},{"location":"demo/DODAS/#cachingondemand-helm-values","text":"Available on github","title":"CachingOnDemand Helm values"},{"location":"demo/DODAS/#prometheus-helm-values","text":"Available on github","title":"Prometheus Helm values"},{"location":"demo/DODAS/#grafana-helm-values","text":"Available on github","title":"Grafana Helm values"},{"location":"demo/DODAS/#create-a-cluster-with-im-client","text":"wget https://github.com/indigo-dc/tosca-templates/tree/k8s_cms/dodas/XCache-demo.yaml im_client.py -a my_auth_file.dat create XCache-demo.yaml","title":"Create a cluster with IM client"},{"location":"demo/DODAS/#create-k8s-secrets-and-configmaps","text":"First log into the k8s master machine and copy there your certificates wherever you prefer e.g.","title":"Create K8s secrets and configmaps"},{"location":"demo/DODAS/#test-functionalities","text":"","title":"Test functionalities"},{"location":"demo/DODAS/#monitor-system-and-dummy-grafana-dashboard","text":"","title":"Monitor system and dummy grafana dashboard"},{"location":"demo/DODAS/#future-improvements","text":"","title":"Future improvements"}]}